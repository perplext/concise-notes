Hello, hello and welcome to. HTTP 1. 1 Must. Die, the. D-Sync. Endgame. Have you ever had a good thing that went a bit too far? Maybe you got more than you bargained for? This is the fourth year that. I've spent researching. HTTP Deez. Incadacs and so. I thought. I knew what. I was going to fight.

The plan was to neatly wrap up a well-understood attack class by finding some weird bugs and niche flaws in obscure systems, the long tail of. Deez. Incadacs. But what. I actually discovered changed both my perspective and the title of this presentation. So today in this session. I'm going to share tools and techniques to enable you to embrace the never-ending horror of. HTTP 1.

1, navigate the. Deez. Inc. Endgame and convince the rest of the world that it's time for. HTTP 1 to die. This started back in 2019 when. I realised that. H1 has a fatal flaw. The isolation between individual. HTTP requests is fundamentally broken. There's no single, reliable way to tell where one request finishes and the next request starts.

So an attacker who finds the tiniest parser discrepancy between the front end and back end server can cause a. Deez. Inc enabling complete site takeover. And with. HTTP 1 being an old, lenient text-based protocol with thousands of different implementations, sending parser discrepancies was not hard. At the time it felt you could hack pretty much any website you wanted to. For example. I was able to show this could be exploited to get persistent control over. Pay. Pal's login page twice.

But we knew what the solution was. If we deployed. HTTP 2 for the upstream connection between the front end and back end, then we could almost eliminate the entire attack class. So six years later, what actually happened?

Well, we didn't do that. We have started using. H2 for the connection between the client and the front end, but then most servers just downgrade that request to. H1 to talk to the back end, and that conversion actually makes the threat worse. And then because we could see, well okay, maybe that's not ideal.

Yes, some things are getting hacked. We decided to bolt on the ultimate security measure, regular expressions. The result is a mess. Here's a classic desync detection probe. You send two requests, and if the target is vulnerable, the first one poisons the connection to the back end, so the second request gets prefixed with the text shown in orange.

Now, if you try this technique on a modern site, chances are the attack will fail even if they're actually vulnerable. That's because it'll get blocked by multiple regular expressions, or the gadget might not work on that target, or it might get missed due to the race condition involving that technique as well. And there's an alternative timeout based approach that avoids the race condition issue, but it gets blocked by even more regular expressions.

So to summarize, over the last six years, the industry has patched the detection methodology. They've patched the scanning tools, but they haven't fixed the actual vulnerability. This has created the desync endgame where everything looks secure until you make the tiniest change to your approach. Let's take a look at an example. A few months ago, I got an email from. Vans saying he'd found a puzzling vulnerability, and would to know my thoughts about what was happening.

The attack that he was trying to do was a pretty standard technique you send a request that looks this, gets downgraded to. H1, poisons the connection to the back end, and the next person to access the affected site gets redirected to his server. If that redirection would get saved in a cache, enabling persistent cache poisoning of. Java. Script files and giving persistent control over every page on the site. And the attack was working absolutely fine except for one small thing, which is that the users that we were seeing being hijacked weren't actually trying to access the target website.

They were trying to access random other websites, including things . Banks, which is a bit weird. So clearly, the server set up doesn't look this, and after seeing what infrastructure the target site was on, my first guess was that this was a desync between the cloud flare front end and. Heroku's reverse proxy, which would mean we were exploiting random websites hosted on the. Heroku platform. And that was plausible, but actually completely wrong.

It's not that simple. So. I realized this when. I tried to replicate his attack, and when it hang on, this attack is getting blocked by cloud flares cache. So the attack isn't actually reaching the back end server, so there's no way this could possibly work.

But when. I corrected his mistake, the attack stopped working. And. I replied back in an email and said it's not working for me, and he said are you sure? And. I went back and. I tried his broken version of the attack and it actually worked fine when you did it that.

So what that means is that. Heroku has nothing to do with this. There was a period of time during which if you tested any website on cloud flare and you did it correctly, you wouldn't find anything. But if you forgot to specify a cache roster, well then you'd cause a desync inside cloud flares own infrastructure enabling you to persistently compromise almost every website using cloud flare.

This is the desync end game. Everything looks secure, but if you do one thing slightly wrong, you can end up hacking 24 million websites. How do bugs that even happen? Well, partly it's the excessive complexity of the systems involved. For example, in this case, we know cloud flare is receiving a request over. H2, converting it to. H1 for internal use, and then converting it back to. H2 again to talk to the upstream, which is obviously not ideal, but the underlying problem comes from the foundation.

There's this idea that. H2P1 is a simple protocol and that makes a secure foundation for whatever arbitrarily complex system you want to build. But the truth is that as soon as you start a proxy. H1, it gets really quite complex. To illustrate that, here's five lies that. I used to personally believe and every one of these is going to be used for an exploit in this presentation. When combined, the last three lies here taken together mean that your proxy needs state just to read in the correct number of bytes from the. TCP socket from the back end server, and you need special casing just to read in the headers before you even get to the body, and the entire response may arrive before you've even received the complete request from the client. This is. H2P1, it's the foundation of the web and it's composed of landmines that routinely expose millions of websites and we've spent six years demonstrating that we're not able to fix it.

It needs to die. So how do we kill it? Well we need to collectively show the world that. H2P1. 1 is insecure and that more decent contacts are always coming. And in the rest of the session, I'll show you how to do that.

First, I'm going to introduce a new toolkit to handle the decent end game and then. I'll introduce two entire new classes of attack exposing a few million more websites. Then. I'll take a focus look at how we can escape this situation and wrap up and take some questions at the back. Now during this session, I'm going to be roughly following the research journey.

So towards the start, we're going to be gaining knowledge but not very much by way of bug bounties. And then towards the end, as the knowledge that we've gained takes us further beyond the state of the art. Businesses are going to rapidly get a lot more profitable. All bounties in this presentation were split equally between everyone involved. 100% of my cut of the bounties has been doubled by port swagger and donated to a local charity and all vulnerabilities in named targets have been resolved.

So how do you win in the end game when there's rejects everywhere? So what you need is a reliable way to detect parser discrepancies. And back in 2021, in a presentation at. Black. Hat. Europe, Daniel. Thatcher gave us one.

And. I was so hyped by this talk that. I took the concept and. I implemented an extra powerful version in. H2B Request. Mugler version 3, which. I've just released. This is an open source burp suite extension fully compatible with the free version. What it does is it uses a broad range of techniques to analyze and classify how the front end and back end are parsing the request and find discrepancies. For example, on this target by looking at simply the status codes, we can see that if we use a space to mask a host header, then we get a unique result that's different to what we get if we just leave the host header out entirely.

And just from that, we can infer that we found a parser discrepancy. This particular one. I will call a visible hidden discrepancy because the masked header is visible to the front end, but hidden from the back end. That scenario can usually be exploited to cause a. CL-0 desync simply by hiding the content length header as shown here. But crucially, if you run into any issues when you're trying to convert it into a desync that, well, you can adapt and overcome those problems because the underlying flaw.

For example, on a different target, they were blocking get requests when they had a body, but. I was able to bypass that simply by switching the method to options. It's that kind of versatility that makes this detection approach so valuable. It also achieves pretty good coverage by combining different headers, permutations and strategies. For example, in this one here, we're still using the host header and we're still using a leading space to hide the host header, but this time we're using a duplicate host header with a malformed value to reveal the issue.

And in this example, that's revealed a desync on a web. VPN used by a bank. This system even lets you predict vulnerabilities. For example, the. RFC says you're allowed to accept a slash. N by itself as a line terminator. And the tool has detected on this target, they're not doing that, which means if you place that server behind an. RFC compliant proxy, there's a good chance it will be vulnerable.

And we were able to trace that issue back to the underlying library and a patch is on the way. The tool also flags a whole load of servers running. IIS behind. Amazon's application load balancer. As you can infer from the server banners here, this discrepancy is a hidden visible one. So it's the opposite way round.

It's the front end that doesn't see the masked header. Now these targets are protected by. AWS desync guardian. And when. I saw that, I thought, this thing's a bit of a pain to deal with.

So. I just put it on the shelf and thought. I'd come back to it later. And while it sat on my shelf, Thomas. Stacey independently discovered the same issue and bypassed desync guardian for full exploit on those targets. The interesting thing is that even now that his bypass has been fixed, they haven't actually fixed the parser discrepancy. So these systems are still vulnerable to header injection.

And to patch that, you have to change a couple of settings on your front end. Now when. I saw that, I went to. AWS and said, hang on, why don't you just change these settings by default? Why not make it secure by default? And they said, well, some of their clients are using ancient. HTTP clients which send malformed requests and they couldn't possibly break those clients.

So that's why things are vulnerable by default. Basically, when you use a cloud proxy, you're importing other people's tentacle debt into your own infrastructure. This finding is where things started to go off on a little bit of a tangent. And. I'm quite sad. I can't name it because we're going to be talking about this one target for the next 10 minutes.

It's another hidden visible scenario and the obvious exploit of using transfer encoding doesn't work. So it forced me to ask, well, how do you cause a decent con this? What happens if you smuggle a content length header? That would be a zero-CLD sync and these are widely regarded as impossible. That's because when the front end doesn't see the content length header, it only forwards the header block to the back end and then the back end ends up timing out while waiting for the body to arrive.

In other words, as soon as you try the attack, you get a server-side deadlock, which is great for taking down servers, but not very useful for decent attacks. Last year, while researching timing attacks, I stumbled on the solution. Whenever. I tried to time how long it would take to load a static file served by engine. X, I would get a negative timestamp because engine. X was responding to my request before. I'd actually finished sending it. This was a massive nuisance at the time, but it showed that sometimes servers respond to incomplete requests, which is something that would let us escape this deadlock.

However, the target isn't running engine. X. It's running. IIS. So how do we make. IIS respond early to our request?

Documentation is a wonderful thing, and this is an example of why. On. Windows, if you use one of the following names for a file or folder, the operating system will think you're referring to a device driver or something that and then just blow up. So what happens if you hit that path on a server?

Well, you hit a special code path which makes the server respond early without waiting for the body to arrive, thereby escaping the deadlock. That means that when the next request arrives, the backend will chop some data off the start of the request and you'll probably get a 400-bad request response. Being this happened was quite an emotional moment for me for two key reasons. Firstly, I'd known about that /con feature for over 10 years, but this was the first time. I'd actually been able to make use of it.

And secondly, over the last six years, while researching desync attacks, I'd seen so many mysterious bad request responses that. I actually made my tool report them as mystery 400 findings, and this was the moment when. I realized that all of those findings were probably actually exploitable. So if you find a zero-CL desync scenario to make it work, the first thing you need is an early response gadget. Next stop, we need to get beyond that bad request response to something more promising.

And you can do that by nesting a second request line inside the header block of the second request and adjusting the content length of the first request to slice out the data before it. This proves that there's a zero-CL desync, but it's not a realistic attack because our victim is not going to conveniently contain a payload to exploit themselves inside their own request. What we need is a way to add our payload to their request instead of just chopping bits off. And we can do that with a double desync.

This is where things get a tiny bit complex. So this is a two-stage attack, where the first request from the attacker causes a zero-CL desync. The weaponizes the second request that also comes from the attacker to cause a. CL0 desync that means the third request from the victim gets exploited. This is the cleanest conceptual way to do it, where you simply chop the headers off the second request. But in practice, this rarely actually works because front ends tend to inject extra headers to requests before forwarding them to the back end, which makes all your length calculations incorrect.

I'm releasing a script which works out the length of the injected header. But unfortunately these headers often contain values your. IP address, which means even if you can identify the offset, your attack will work perfectly for you, but then break as soon as you give it to someone else. Fortunately, there's a better way. Most servers inject headers at the end of the header block. So that means that if your payload starts before that, your attack, although it now looks quite ugly, will be a lot more reliable.

In this example here, I'm just changing the desync with an input reflection on the website to reveal the value of the injected header, but by then changing that with the head technique to merge two responses into one, I was able to serve malicious. Java. Script to random live users of the website. There was a race condition involved in this, which caused me a huge amount of pain. But basically it just meant the attack would work reliably as long as you were sending a few hundreds of requests per second.

Now things got a bit involved there, but the good news is we've just published a web security academy lab. So you can practice that technique yourself on a live system for free. And in about a week's time, are we attempting to do a live demo showing how to solve that lab?

Now with that technique, we were able to exploit a reasonable number of targets with the best payout coming from. X and. S. And notably, most of those targets were vulnerable because they had a web application firewall. They would have been secure if they had not deployed that web application firewall in front of their website. Now at this point, I thought, great, we're done.

The desync threat is finally fully mapped, and any future issues will be weird niche problems rather than big class breaks, and. I can finally start to research something else. This is a mistake that. I make every single year. It took the next discovery for me to finally realize the truth, which is that more desync attacks are always on the way.

Back in 2022, I tried out exploiting the expect header, but. I didn't find anything, but as it turns out, I wasn't looking close enough. The first clue that the expect header is something special is that it instantly broke turbo intruder, which is my hate to be client, and fixing it introduced a ton of complexity. And if something introduces complexity for a client or for a server, it's always way worse for a proxy.

Expect is supposed to break sending a request into a two-part process so the client can bail early in order to save bandwidth. No browsers support it, but because it's in the. RFC and has been since forever, virtually all servers attempt to support it. But because it introduces statefulness into a core area of the logic that didn't previously need it, it raises a load of questions about edge cases.

And basically, when you send this header, things go wrong. So for example, on this site, the head request, which is a special case, works fine. And the expect header, which is a special case, works fine.

But you combine the two and the server forgets about the first special case, tries to read too many bytes from the backend server and deadlocks. Now that's the kind of flaw that you could predict, but you'll also find some more unexpected discoveries, such as the fact that on multiple different web servers, if you send the expect header, they just leak memory, including secret keys. When. I went to report that particular one, I found they'd just deleted their bug bounty program, which is not ideal. Now because the expect header triggers two header blocks and all other responses have one, it often breaks attempts by front end servers to strip sensitive headers in order to hide them from end users, and thereby it reveals internal headers.

And this technique works on everything on the. Netlify. CDN. And when. I went to report that to them, they said, this is actually a feature, but they did pay me for it. But. I don't think they fixed it, so if you need any of those values, it might come in useful.

Now around this time, I got a message from a small team of full time bounty hunters who had also noticed the expect header making interesting things happen. And this kind of made me panic, because in general, a research collision is not good news. In fact, we lost a potential. DEF CON presentation this year on the. SAML exploitation, thanks to a different research collision. But this issue hadn't been published yet, and since. I knew these people from their research on. T0 request smuggling, and they already knew the technique, I figured we might as well team up with my help, hopefully they'd make more money, and with their help, I should end up with more case studies.

This was a good decision. We quickly found that by sending a normal valid expect header, you get a zero. CLD sync on a whole lot of targets, so. I'm going to focus on two in particular. On this one, this was just a great interaction, because. T-Mobile gave us a $12,000 payout even though it only affected a single pre-production server. And on this target, it only worked if you obfuscated the expect header value by adding a single character, and the vulnerable domain holds the attachments to the vulnerability reports sent to. Git. Labs bug bounty program, which obviously will be quite nice to see some of those, wouldn't it?

So on this target, we opted to go for response. Q poisoning. This is a truly glorious attack where you smuggle two complete requests, which makes the server lose track of which response is meant to go to who, and just sends everyone random responses intended for other users. Now it can be quite hard on low traffic servers that target, but we persisted, and after sending 27,000 requests, we got access to someone else's private vulnerability report and a respectable $7,000 payout of. Git.

Lab. Using similar techniques elsewhere, we got some really quite nice bounties that. I can't talk about, taking the total to over 100k. The expect header also causes. CL0D syncs, basically if you send expects everything that might go wrong, dots. So this one affects every single website on our favourite, the. Netlify. CDN.

And once again we've got response. Q poisoning, but this time it's affecting the entire. CDN, so it's hijacking a continuous stream of responses from over a million different websites. Now we found this vulnerability on a particular. Netlify user that has their own bounty program, because we were hijacking responses from so many websites, we couldn't actually get a response from that target, which was a bit annoying because it meant we couldn't report it to them. And then these humans seriously stopped working, so we said okay let's just report it to. Netlify.

And they said websites using. Netlify are not in scope, and didn't pay us anything, which was a bit disappointing. And normally when. I have a bad bounty experience, I don't talk about it because it just distracts everyone from the technical content, but this particular one is useful context as to something that happened shortly later. This is the last decent attack in this presentation and it's a special one. It gave us full control over auth. Lastpass.

Com, letting us serve content from entirely different websites on there, and letting us their maximum bounty, but there was more to it. This technique worked on a large number of websites. There's a few select sites where if you find that you can hack them, that you're on to something really good. And with that technique, there was significant evidence that we could have used it to hack example.

Com. This would have been seriously cool because. I bet they get a lot of interesting traffic. Unfortunately, they don't have a. VDP or bounty program or anything that, so it would have been illegal to prove it, and we will never know for sure. Hopefully. I can persuade them to set them up to set one up and then have another go next year.

Now most of the targets we use in the. Akamai. CDN, so we kind of had a choice here. Could we report the vulnerability to every affected company individually, or should we report it to. Akamai only, and if they react the same way as. Netlify did, not get paid? You can probably guess which option was more appealing. It was a tricky decision for me because having a good relationship with a. CDN over the long term is important, but when. I collaborate with bounty hunters, I want them to make more money as a result of working with me, not less.

So, looking at the specifics of that finding, I said to the bounty hunters, "Well, you would have found this without me, so. I'm going to sit this one out. You report it, don't put me on the report, and. I won't take a cut of the payouts. " This is a decision that. I still have somewhat mixed feelings about, because they went on to over $200,000 from this issue.

Overall, the reports were well received, but things didn't go entirely smoothly. It turned out that the vulnerability was actually fully inside. Akamai's infrastructure, so they just got hammered with support tickets from their clients, and it seemed to be taking them a while to fix it, because it had been reported to so many different companies, I got stressed about the technique potentially leaking before it was fixed, so. I reached out to. Akamai directly and got a $9,000 bounty in the process for that, but although they got a hot fix out for some clients quite fast, it still took over 65 days from that point to fully resolve the vulnerability. Overall, that one was quite a stressful experience, but at least. I got some good. US dollar back evidence of the dangers posed by. H2B1.

1, as it took the total bounties earned to over 350 k. So, all the attacks. I've shown you are exploiting implementation flaws, so it might seem weird to say that the solution is to delete the protocol, but they all come from the same single protocol flaw, which is the lack of robust request isolation. It's compounded by two key factors, one is that. H2B1 is not simple if you're trying to proxy it, and number two is that we basically don't want to patch. H2B1 the right way, because it would break support for legacy clients. These factors all just combine to mean exactly one thing, which is that more decent attacks are always coming. To escape, we need to use. H2 or 3.

It's not the perfect protocol by any means, it's obnoxiously complex and it's binary so you can't just read it, but it doesn't have the fatal flaw, so what that means is that the inevitable implementation bugs are going to be much lower impact. Of course, don't use. H2B2 downgrading, that just makes everything worse. So to kill request muggling, make sure your origin server supports. H2, and then turn on upstream. H2 on the front end, and the protocol will just take care of everything else.

Note that you do not need to turn off. H1 for clients. Because those connections are not shared between different users, which makes them significantly less dangerous. Unfortunately, some major players, such as. Nginx, Akamai, Cloudfront, and. Farsley, don't support upstream. H2 yet, so if you're stuck on one of those, there's some other approaches you can take to try to mitigate these issues, but. I do not regard any of these as adequate long-term solutions.

It's mostly about surviving until you can deploy. H2. The only way to use. H1 safely is don't have a front end server. We will be releasing more updates to. Request. Smuggler 3 as more techniques become known, so at least if you're doing regular scans with that, you'll find out relatively fast if you're exposed. Now, this is where. I need your help. The number one problem that we've got is that people think upstream. H1 is secure, so together we need to show the world the truth.

We need to show how broken it is by exploiting systems, breaking them, and sharing our findings with the message that more decent contacts are coming. And because we're in the decent end game, and there's inevitably going to be future waves of regular expressions, regular expressions dedicated to breaking every detection method we can think of, take my tools and techniques, but adapt them. Make small changes, and you'll discover a lot more vulnerabilities. For me, the key big research question is, what's the next expect?

Well, other features in. H2P1. 1 have serious decent potential. The cool thing about this question is that. H1 is so poorly understood that there's actually tons defined even beyond the realm of decent attacks. For example, it's well known that you can use the trace method to leak internal headers if the back end supports it, but what if the back end has disabled trace?

Does that mean they're secure? Well, unfortunately not, because using the max forwards header, you can target an arbitrary intermediary server with that request instead. So unless they've configured every server in the chain, the technique still works.

Basically, H2P1 is an absolute gold mine for all kinds of research. There's a whole load of further reading available online, and you can find everything you need linked from. H2P1 musti. Com. Note this website is hosted on. Cloudfront, and that means it is actually using upstream. H2P1, but if you can get a decent coin, we will pay you a bonus on your bug bounty.

The three key things to take away are that this is not the end. More decent attacks are always coming. If we want a secure web, H2P1 needs to die, and together we can kill it. I'm going to take some questions at the back. I also have some t-shirts to hand out.

Don't forget to follow me on social media, and if you have any more questions after that, feel free to chuck me an email. Thank you for listening.